import os
import re
import spacy
import yake
import json
import requests
from termcolor import colored
from dotenv import load_dotenv
# Charger automatiquement la cl√© depuis un .env local
load_dotenv()
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

MODELES_ARCH = {
    "ARCH_NOTE": "mistral-medium-latest"
}
# Chargement global de spaCy
nlp = spacy.load("fr_core_news_md")



def extraire_json(texte):
    """
    Extrait la premi√®re structure JSON valide d‚Äôun texte brut (souvent encadr√©e par ```json ... ``` ou bruit√©e).
    """
    try:
        # Supprime d√©corations Markdown √©ventuelles
        texte = texte.strip()
        if texte.startswith("```json"):
            texte = texte[7:]
        if texte.endswith("```"):
            texte = texte[:-3]
        # Supprime tout avant le premier {
        start = texte.find("{")
        texte = texte[start:]
        return json.loads(texte)
    except Exception as e:
        print("‚ö†Ô∏è Erreur dans extraire_json :", str(e))
        return {}



# Pond√©rations officielles des balises
PONDERATIONS_BALISES = {
    "#pivot": 2,
    "#conclusion": 2,
    "#d√©tour": 1,
    "#encha√Ænement": 1,

    "#id√©e_cl√©": 3,
    "#rupture": 3,
    "#hypoth√®se": 2,
    "#mod√®le": 2,

    "#causalit√©": 2,
    "#finalit√©": 2,
    "#temps": 1,
    "#lieu": 1,

    "#√©mergence": 4,
    "#urgence": 3,
    "#√†_relire": 2,
    "#dormance": 1
}

PROMPT_RESUME_ARCH_V1 = """
Tu es un assistant de cognition augment√©e.

Tu vas recevoir une fiche textuelle. Elle peut contenir un ou plusieurs paragraphes, ou une suite de fragments discontinus. Ton objectif est double :

1. Produire un **r√©sum√© structur√©**, fluide et fid√®le, entre 400 et 500 tokens mais surtout pas moins.
Ce r√©sum√© ne doit pas se contenter de condenser, mais doit restituer l‚Äôarchitecture logique du texte, ses tensions internes (paradoxe, incertitude, opposition, √©mergence‚Ä¶) et ses encha√Ænements s√©mantiques (cause, cons√©quence, contraste, hypoth√®se).  
   ‚û§ S‚Äôil y a des contrastes ou des limites dans le propos, formule-les clairement.  
   ‚û§ Si une id√©e para√Æt cruciale ou ambivalente, donne-lui un relief particulier.
   ‚û§ Fais comme si tu √©crivais pour un syst√®me de cognition r√©flexive, et non pour un lecteur humain.
   ‚û§ N‚Äôimite pas m√©caniquement les exemples. 
   ‚û§ Tu peux varier la longueur (jusqu‚Äô√† 500 tokens) et la tension cognitive selon la complexit√© du texte. 
   ‚û§ Si une opposition, une limite ou un paradoxe appara√Æt, mets-le en valeur.

2. Produire ensuite un **m√©ta-r√©sum√©**, en deux lignes maximum. Ce m√©ta-r√©sum√© ne doit **jamais redire le r√©sum√©**, mais capter la **dynamique de pens√©e** du texte (polarit√©, d√©s√©quilibre, tension implicite, synth√®se incompl√®te). Il doit √™tre stylis√©, mais sobre.

Ta r√©ponse doit √™tre au format JSON strict, comme ceci :
{
  "resume": "‚Ä¶",
  "meta_resume": "‚Ä¶"
}

Langue : fran√ßais.  
N‚Äôutilise jamais de bullet points.  
N‚Äôajoute ni titre, ni commentaire autour du JSON.  


‚Äî

√Ä toi maintenant. Voici la fiche √† analyser :"
‚Äî


"""


PROMPT_UNIFIE_ARCH_V1 = """
Tu es un agent cognitif expert en cognition augment√©e.

Tu vas recevoir une fiche d√©j√† fragment√©e. Chaque fragment correspond √† une unit√© de sens issue d‚Äôune segmentation algorithmique (type YAKE). Tu ne dois pas les modifier, ni les reformuler.

Ta mission est double :

1. Produire un **r√©sum√© structur√©** fid√®le et fluide (400 √† 500 tokens, jamais moins). Tu dois reconstituer le fil logique implicite du texte en concat√©nant mentalement les fragments.  
   ‚û§ Ce r√©sum√© doit refl√©ter les tensions internes (paradoxes, oppositions, contrastes, √©mergences‚Ä¶).  
   ‚û§ Il ne doit pas √™tre une simple condensation, mais une **mise en architecture** du propos.  
   ‚û§ Tu √©cris pour un syst√®me de cognition r√©flexive. Pas de simplification inutile.  
   ‚û§ Si une id√©e est ambivalente, souligne-la. Si une opposition est latente, explicite-la.

2. Produire un **m√©ta-r√©sum√©**, en deux lignes maximum.  
   ‚û§ Il ne doit **jamais r√©p√©ter le r√©sum√©**.  
   ‚û§ Il doit faire sentir la **dynamique mentale ou la polarit√© du propos** (instabilit√©, tension implicite, d√©s√©quilibre, esquive‚Ä¶).  
   ‚û§ Il doit √™tre stylis√©, mais sobre.

3. **Attribuer un balisage typologique multistrate** √† chaque fragment (sans modifier leur contenu).

Ta r√©ponse finale doit √™tre au **format JSON strict**, structur√© ainsi :
{
  "resume": "‚Ä¶",
  "meta_resume": "‚Ä¶",
  "fragments": [
    {
      "fragment_id": "F001",
      "structurelle": "#‚Ä¶",
      "conceptuelle": "#‚Ä¶",
      "r√©f√©rentielle": ["#‚Ä¶"],
      "vitale": "#‚Ä¶",
      "√©l√©ments_visuels": "‚Ä¶",
      "justification_balise": "‚Ä¶"
    },
    ‚Ä¶
  ]
}

‚ö†Ô∏è Respecte exactement ce format (7 cl√©s pour chaque fragment).  
Langue : fran√ßais.  
Aucune d√©coration Markdown. Pas de texte avant ou apr√®s le JSON. Pas de commentaire. Aucune bullet point.

‚Äî

Voici maintenant les fragments (n‚Äôen modifie aucun) :
"""




def g√©n√©rer_tags_systeme(texte):
    mots_cles = ["m√©moire", "mod√®le", "heuristique", "temps", "interaction"]
    return [mot for mot in mots_cles if mot in texte.lower()]

def consolider_tags_groupes(groupes):
    return groupes  # Simulation

def fusionner_tags(tags_manuels, tags_systeme):
    return list(set(tags_manuels)) + [t for t in tags_systeme if t not in tags_manuels]

def extraire_tags_lexicaux(texte, langue="fr", nb_mots_max=5):
    kw_extractor = yake.KeywordExtractor(lan=langue, n=1, top=nb_mots_max)
    mots_cles = kw_extractor.extract_keywords(texte)
    return [mot for mot, score in mots_cles]

def analyser_fragment(fragment, tag_manuels_fiche):
    doc = nlp(fragment)

    verbes = list(set([token.lemma_ for token in doc if token.pos_ == "VERB"]))

    noms_propres = set()
    for ent in doc.ents:
        if ent.label_ == "PER":
            noms_propres.add(ent.text)
    for token in doc:
        if token.pos_ == "PROPN":
            noms_propres.add(token.text)

    tags_lexicaux = extraire_tags_lexicaux(fragment)

    balises_structurelles = ["#pivot"] if "ramassa" in fragment else []
    balises_conceptuelles = ["#id√©e_cl√©"] if "chant" in fragment else []
    balises_r√©f√©rentielles = ["#lieu"] if "barricade" in fragment else []
    balises_vitales = ["#√©mergence"] if "sourire" in fragment else []

    return {
        "fragment": fragment,
        "tags_lexicaux": tags_lexicaux,
        "noms_propres": list(noms_propres),
        "verbes": verbes,
        "structurelle": balises_structurelles,
        "conceptuelle": balises_conceptuelles,
        "r√©f√©rentielle": balises_r√©f√©rentielles,
        "vitale": balises_vitales,
        "tag_manuels_fiche": tag_manuels_fiche
    }

def calculer_score_balises(balises):
    """
    Calcule un score √† partir des balises fournies.
    balises = {
        "structurelle": [...],
        "conceptuelle": [...],
        "r√©f√©rentielle": [...],
        "vitale": [...]
    }
    """
    total = 0
    for famille in ["structurelle", "conceptuelle", "r√©f√©rentielle", "vitale"]:
        for tag in balises.get(famille, []):
            total += PONDERATIONS_BALISES.get(tag, 0)
    return total


def generer_matrice_cognitive(texte, tag_manuels_fiche):
    """
    G√©n√®re une matrice cognitive multistrate √† partir d‚Äôun texte brut.
    - Segmente en fragments
    - Extrait : tags lexicaux (YAKE), noms propres, verbes
    - Applique les balises typ√©es
    - Injecte tag_manuels_fiche dans chaque fragment
    """
    doc = nlp(texte)
    matrice = []

    
     #√©limination du bruit dans les fragments. Le chiffre donne le nombre de caract√®res qui donne un fragment exploitable
    fragments = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]
    for i, fragment in enumerate(fragments):
        doc_fragment = nlp(fragment)
        id_frag = f"F{i+1:03d}"

        # YAKE
        tags_lexicaux = extraire_tags_lexicaux(fragment)

        # Noms propres (PROPN)
        noms_propres = [token.text for token in doc_fragment if token.pos_ == "PROPN"]

        # Verbes
        verbes = [token.lemma_ for token in doc_fragment if token.pos_ == "VERB"]

        # Score pond√©r√©
        balises_dict = {
            "structurelle": structurelle,
            "conceptuelle": conceptuelle,
            "r√©f√©rentielle": r√©f√©rentielle,
            "vitale": vitale
        }
        score = calculer_score_balises(balises_dict)

        matrice.append({
            "id": id_frag,
            "fragment": fragment,
            "tags_lexicaux": tags_lexicaux,
            "noms_propres": noms_propres,
            "verbes": verbes,
            "structurelle": structurelle,
            "conceptuelle": conceptuelle,
            "r√©f√©rentielle": r√©f√©rentielle,
            "vitale": vitale,
            "tag_manuels_fiche": tag_manuels_fiche,
            "score": score,
            "moteur_utilis√©": "GPT‚Äë4.1 Mini (simul√©)",
            "niveau_confiance": 0.95,
            "appel_externe_pr√©vu": False,
            "agent_final_sugg√©r√©": "Aucun",
            "justification_balise": "",
            "√©l√©ments_visuels": []
        })
    
    return matrice


def generer_resume_arch(fiche):
    contenu = fiche.get("texte", "").strip()
    if not contenu:
        raise ValueError("Fiche vide : champ 'texte' introuvable ou vide")

    messages = [
        {
            "role": "user",
            "content": PROMPT_RESUME_ARCH_V1.strip()
        },
        {
            "role": "user",
            "content": contenu
        }
    ]

    try:
        print("üß† Appel principal : GPT-4o pour r√©sum√© structur√©...")
        result = appel_gpt4o(messages)
        print("üßæ [DEBUG] R√©sultat LLM avant parsing :", repr(result))

        # --- Nettoyage Markdown √©ventuel avant parsing JSON ---
        if result.strip().startswith("```json"):
            result = result.strip()[7:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()
        elif result.strip().startswith("```"):
            result = result.strip()[3:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()

        try:
            resume_data = json.loads(result)
        except Exception as err_json:
            print("‚ùå Erreur parsing JSON (GPT-4o r√©sum√©) :", err_json)
            print("üßæ R√©ponse brute :", result)
            raise RuntimeError(f"Erreur parsing JSON r√©sum√© (GPT-4o) : {err_json}")

        fiche["resume"] = resume_data.get("resume", "")
        fiche["meta_resume"] = resume_data.get("meta_resume", "")
        fiche["moteur_resume"] = "GPT‚Äë4o"

    except Exception as e:
        print("‚ö†Ô∏è Erreur GPT‚Äë4o, fallback Mistral activ√© :", str(e))
        try:
            resume_data = generer_resume_mistral(contenu)
            fiche["resume"] = resume_data.get("resume", "")
            fiche["meta_resume"] = resume_data.get("meta_resume", "")
            fiche["moteur_resume"] = "Mistral"
        except Exception as e2:
            print("‚ùå Erreur fallback Mistral :", str(e2))
            fiche["resume"] = "Erreur lors du r√©sum√©."
            fiche["meta_resume"] = "N/A"
            fiche["moteur_resume"] = "Erreur"
  # ‚û°Ô∏è AJOUTE cette ligne √† la toute fin :
    return fiche

def generer_resume_mistral(contenu):
    headers = {
        "Authorization": f"Bearer {MISTRAL_API_KEY}",
        "Content-Type": "application/json"
    }

    messages = [
        {
            "role": "user",
            "content": PROMPT_RESUME_ARCH_V1.strip()
        },
        {
            "role": "user",
            "content": contenu
        }
    ]

    data = {
        "model": "mistral-medium-latest",
        "messages": messages,
        "temperature": 0.2,
        "max_tokens": 1500,
        "top_p": 1.0
    }

    response = requests.post("https://api.mistral.ai/v1/chat/completions", headers=headers, json=data)
    response.raise_for_status()
    result_json = response.json()
    resume_data_str = result_json["choices"][0]["message"]["content"]
    print("üßæ [DEBUG] R√©sultat Mistral avant parsing :", repr(resume_data_str))

    # Ajout du parsing robuste :
    try:
        resume_data = json.loads(resume_data_str)
    except Exception as err_json:
        print("‚ùå Erreur parsing JSON (Mistral r√©sum√©) :", err_json)
        print("üßæ R√©ponse brute :", resume_data_str)
        raise RuntimeError(f"Erreur parsing JSON r√©sum√© (Mistral) : {err_json}")
    return resume_data

def generer_balises_mistral(fragments):
    """
    Envoie une liste de fragments textuels √† Mistral pour g√©n√©rer un balisage typologique structur√©.
    Retourne une liste de dictionnaires avec 7 cl√©s par fragment.
    """
    import os
    import json
    import requests

    MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
    if not MISTRAL_API_KEY:
        raise RuntimeError("Cl√© API Mistral manquante")

    if not fragments:
        return []

    prompt_instructions = """
Tu es un agent cognitif expert en analyse textuelle.
Ta mission est d‚Äôanalyser chaque fragment re√ßu et d‚Äôattribuer un balisage multistrate dans un format JSON strictement norm√©.

Ta r√©ponse doit √™tre une liste JSON (array), contenant un objet par fragment avec exactement 7 cl√©s dans l‚Äôordre suivant :

1. "fragment_id" : Code identifiant du fragment (ex. "F001", "F002", etc.).
2. "structurelle" : Une balise parmi : #introduction, #encha√Ænement, #pivot, #d√©tour, #conclusion, #fragment_incomplet.
3. "conceptuelle" : Une balise parmi : #id√©e_cl√©, #mod√®le, #hypoth√®se, #description, #√©v√©nement, #rupture, #n√©ant.
4. "r√©f√©rentielle" : Liste de balises parmi : #temps, #lieu, #causalit√© (peut √™tre vide).
5. "vitale" : Une balise parmi : #urgence, #√©mergence, #dormance.
6. "√©l√©ments_visuels" : M√©taphore ou image stylis√©e repr√©sentant le fragment.
7. "justification_balise" : 2 √† 3 lignes justifiant les choix pr√©c√©dents.

Langue : fran√ßais.
Format de sortie : JSON strict sans titre ni d√©cor.

Voici les fragments √† analyser :
""".strip()

    texte_fragments = "\n".join([f'{i+1}. {frag}' for i, frag in enumerate(fragments)])

    messages = [
        {"role": "user", "content": prompt_instructions},
        {"role": "user", "content": texte_fragments}
    ]

    headers = {
        "Authorization": f"Bearer {MISTRAL_API_KEY}",
        "Content-Type": "application/json"
    }

    data = {
        "model": "mistral-medium",
        "temperature": 0.2,
        "messages": messages
    }

    try:
        response = requests.post("https://api.mistral.ai/v1/chat/completions", headers=headers, json=data)
        response.raise_for_status()
        result = response.json()["choices"][0]["message"]["content"]

        # Nettoyage des balises markdown √©ventuelles
        if result.strip().startswith("```json"):
            result = result.strip()[7:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()
        elif result.strip().startswith("```"):
            result = result.strip()[3:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()

        print("üßæ R√©ponse Mistral (nettoy√©e) :", result)
        print("üßæ [DEBUG] R√©sultat balises Mistral avant parsing :", repr(result))

        try:
            balises_mistral = json.loads(result)
        except Exception as err_json:
            print("‚ùå Erreur parsing JSON (Mistral balises) :", err_json)
            print("üßæ R√©ponse brute :", result)
            raise RuntimeError(f"Erreur parsing JSON balises typologiques (Mistral) : {err_json}")

        return balises_mistral

    except Exception as e:
        print("‚ùå Erreur appel Mistral balises :", e)
        raise


def generer_resume_et_balises_unifie(fragments):
    """
    Envoie les fragments √† GPT‚Äë4o dans un seul appel unifi√© (r√©sum√© structur√© + balises typologiques).
    Retourne : (resume, meta_resume, liste_balises)
    """

    prompt = [
        {"role": "user", "content": PROMPT_UNIFIE_ARCH_V1.strip()},
        {"role": "user", "content": json.dumps(fragments, ensure_ascii=False, indent=2)}
    ]

    try:
        print("üìå Appel GPT‚Äë4o unifi√© (r√©sum√© + balises typologiques)‚Ä¶")
        resultat = appel_gpt4o(prompt, max_tokens=4000)
        print("üì• R√©ponse re√ßue ‚úÖ")
    except Exception as e:
        print(f"‚ö†Ô∏è GPT‚Äë4o indisponible, bascule vers Mistral‚Ä¶ ({e})")
        try:
            resultat = appel_mistral(prompt, max_tokens=4000)
        except Exception as fallback_e:
            print(f"‚ùå √âchec Mistral √©galement : {fallback_e}")
            return None, None, []

    try:
        # Adaptatif selon le type de retour (dict d√©j√† pars√© ou string √† parser)
        if isinstance(resultat, dict):
            data = resultat
        else:
            data = extraire_json(resultat)

        resume = data.get("resume", "").strip()
        meta_resume = data.get("meta_resume", "").strip()
        fragments_balises = data.get("fragments", [])
        titre_fiche = data.get("titre_fiche", "").strip()
        return resume, meta_resume, fragments_balises, titre_fiche
        


    except Exception as parse_error:
        print(f"‚ùå Erreur de parsing JSON unifi√© : {parse_error}")
        return None, None, []


def afficher_matrice_console(matrice):
    """
    Affiche proprement une matrice cognitive avec couleurs selon les balises.
    """
    print("\nüß† MATRICE COGNITIVE ‚Äì AFFICHAGE CONSOLE")
    for fragment in matrice:
        id_frag = fragment.get("id", "???")
        texte = fragment.get("fragment", "").strip()
        if not texte:
            continue

        print(f"\nüîπ fragment {id_frag} : {colored(texte, 'cyan')}")
        
        tags_struct = ", ".join(fragment.get("structurelle", []))
        tags_concept = ", ".join(fragment.get("conceptuelle", []))
        tags_ref = ", ".join(fragment.get("r√©f√©rentielle", []))
        tags_vie = ", ".join(fragment.get("vitale", []))

        if tags_struct:
            print(colored(f"üü† Structurelle : {tags_struct}", 'yellow'))
        if tags_concept:
            print(colored(f"üîµ Conceptuelle : {tags_concept}", 'blue'))
        if tags_ref:
            print(colored(f"üü£ R√©f√©rentielle : {tags_ref}", 'magenta'))
        if tags_vie:
            print(colored(f"üî¥ Vitale : {tags_vie}", 'red'))

        # Score en gris clair
        score = fragment.get("score", 0)
        print(colored(f"Score heuristique : {score}", 'white'))

def explorer_matrice_console(matrice, score_min=None, tag_contenu=None, afficher_tout=False):
    """
    Exploration console cibl√©e de la matrice cognitive.
    - score_min : n'affiche que les fragments avec score ‚â• score_min
    - tag_contenu : n'affiche que les fragments contenant ce tag (dans structurelle, conceptuelle, r√©f√©rentielle ou vitale)
    - afficher_tout : ignore tous les filtres
    """
    print("\nüîç EXPLORATION DE LA MATRICE COGNITIVE\n")

    nb = 0
    for fragment in matrice:
        texte = fragment.get("fragment", "").strip()
        score = fragment.get("score", 0)
        id_frag = fragment.get("id", "??")

        tous_les_tags = fragment.get("structurelle", []) + fragment.get("conceptuelle", []) + fragment.get("r√©f√©rentielle", []) + fragment.get("vitale", [])

        if afficher_tout:
            pass
        elif tag_contenu:
            if tag_contenu not in tous_les_tags:
                continue
        elif score < score_min:
            continue

        print(f"üß© fragment {id_frag} ‚Äì Score : {score}")
        print(f"‚úèÔ∏è  {texte}")
        if tous_les_tags:
            print(f"üè∑Ô∏è Tags : {', '.join(tous_les_tags)}")
        print()
        nb += 1

    print(f"‚úÖ {nb} fragment(s) affich√©(s)\n")


def appel_gpt4o(messages, model="gpt-4o", temperature=0.4, max_tokens=600):
    url = "https://api.openai.com/v1/chat/completions"
    api_key = os.getenv("OPENAI_API_KEY")
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens
    }

    # Affichage debug pour voir le JSON envoy√©
    print("üì§ Envoi √† OpenAI :\n", json.dumps(data, indent=2, ensure_ascii=False))

    try:
        response = requests.post(url, headers=headers, json=data, timeout=40)
        print("üì• Statut HTTP :", response.status_code)
        print("üì• R√©ponse brute :", response.text[:500])

        if response.status_code != 200:
            raise RuntimeError(f"OpenAI HTTP {response.status_code} : {response.text}")

        # V√©rification : la r√©ponse doit √™tre un vrai JSON
        try:
            reponse_json = response.json()
        except Exception as e:
            print("‚ùå Erreur parsing JSON :", e)
            print("üßæ R√©ponse brute :", response.text)
            raise RuntimeError(f"Erreur parsing JSON : {e}")

        # V√©rification : la cl√© attendue existe
        if ("choices" not in reponse_json or
            not isinstance(reponse_json["choices"], list) or
            "message" not in reponse_json["choices"][0] or
            "content" not in reponse_json["choices"][0]["message"]):
            raise RuntimeError(f"R√©ponse inattendue de OpenAI : {response.text}")

        return reponse_json["choices"][0]["message"]["content"]

    except Exception as e:
        print("‚ùå Exception lors de l'appel GPT‚Äë4o :", str(e))
        raise RuntimeError(f"Erreur appel OpenAI GPT-4o : {e}")

def generer_balises_typologiques(fragments):
    """
    Applique GPT‚Äë4o pour extraire les balises typologiques de chaque fragment.
    Fallback Mistral si l‚Äôappel √©choue.
    """
    import json
    fragments_clean = [
        {"id": f"F{i+1:03d}", "contenu": frag.strip()}
        for i, frag in enumerate(fragments)
        if frag.strip()
    ]

    instruction = """
Tu es un agent cognitif expert en analyse textuelle.
Ta mission est d‚Äôanalyser chaque fragment re√ßu et d‚Äôattribuer un balisage multistrate dans un format JSON strictement norm√©.

Ta r√©ponse doit √™tre une liste JSON (array), contenant un objet par fragment avec exactement 7 cl√©s dans l‚Äôordre suivant‚ÄØ:

1. "fragment_id" : Code identifiant du fragment (ex. "F001", "F002", etc.).
2. "structurelle" : Une balise parmi‚ÄØ: #introduction, #encha√Ænement, #pivot, #d√©tour, #conclusion, #fragment_incomplet.
3. "conceptuelle" : Une balise parmi‚ÄØ: #id√©e_cl√©, #mod√®le, #hypoth√®se, #description, #√©v√©nement, #rupture, #n√©ant.
4. "r√©f√©rentielle" : Liste de balises parmi‚ÄØ: #temps, #lieu, #causalit√© (peut √™tre vide).
5. "vitale" : Une balise parmi‚ÄØ: #urgence, #√©mergence, #dormance.
6. "√©l√©ments_visuels" : M√©taphore ou image stylis√©e repr√©sentant le fragment.
7. "justification_balise" : 2 √† 3 lignes justifiant les choix pr√©c√©dents.

Langue‚ÄØ: fran√ßais.
Format de sortie‚ÄØ: JSON strict sans titre ni d√©cor.
"""

    prompt = [
        {"role": "user", "content": instruction},
        {"role": "user", "content": json.dumps(fragments_clean, ensure_ascii=False, indent=2)}
    ]

    try:
        print("üîç Appel GPT‚Äë4o pour balises typologiques‚Ä¶")
        result = appel_gpt4o(prompt, max_tokens=3000)

        # --- Nettoyage √©ventuel des balises markdown
        if result.strip().startswith("```json"):
            result = result.strip()[7:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()
        elif result.strip().startswith("```"):
            result = result.strip()[3:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()

        print("üßæ [DEBUG] R√©sultat balises LLM avant parsing :", repr(result))
        try:
            fragments_annotes = json.loads(result)
        except Exception as err_json:
            print("‚ùå Erreur parsing JSON (GPT-4o balises) :", err_json)
            print("üßæ R√©ponse brute :", result)
            raise RuntimeError(f"Erreur parsing JSON balises typologiques (GPT-4o) : {err_json}")

        # Format standard
        matrice = []
        for i, original in enumerate(fragments_clean):
            balises = fragments_annotes[i] if i < len(fragments_annotes) else {}

            structurelle = [balises.get("structurelle")] if balises.get("structurelle") else []
            conceptuelle = [balises.get("conceptuelle")] if balises.get("conceptuelle") else []
            r√©f√©rentielle = balises.get("r√©f√©rentielle", [])
            vitale = [balises.get("vitale")] if balises.get("vitale") else []

            score = calculer_score_balises({
                "structurelle": structurelle,
                "conceptuelle": conceptuelle,
                "r√©f√©rentielle": r√©f√©rentielle,
                "vitale": vitale
            })

            matrice.append({
                "id": original["id"],
                "fragment": original["contenu"],
                "structurelle": structurelle,
                "conceptuelle": conceptuelle,
                "r√©f√©rentielle": r√©f√©rentielle,
                "vitale": vitale,
                "√©l√©ments_visuels": balises.get("√©l√©ments_visuels", ""),
                "justification_balise": balises.get("justification_balise", ""),
                "score": score,
                "moteur_utilis√©": "GPT‚Äë4o",
                "balises": balises
            })

        return matrice

    except Exception as e:
        print("‚ö†Ô∏è Erreur GPT‚Äë4o pour balises typologiques :", str(e))
        print("üîÅ Fallback Mistral activ√©.")
        return generer_balises_mistral([frag["contenu"] for frag in fragments_clean])


    except Exception as e:
        print("‚ö†Ô∏è Erreur GPT‚Äë4o pour balises typologiques :", str(e))
        print("üîÅ Fallback Mistral activ√©.")
        return generer_balises_mistral([frag["contenu"] for frag in fragments_clean])

def charger_fiche_json(nom_fichier):
    """
    Charge une fiche ARCH √† partir d'un nom de fichier JSON.
    Retourne la matrice cognitive (champ "memo_r") ou une liste vide si erreur.
    """
    import os
    import json
    chemin = os.path.join("fiches", nom_fichier)
    if not os.path.exists(chemin):
        print(f"‚ùå Fichier introuvable : {chemin}")
        return []

    try:
        with open(chemin, "r", encoding="utf-8") as f:
            fiche = json.load(f)
        return fiche.get("memo_r", [])
    except Exception as e:
        print(f"‚ùå Erreur de chargement JSON : {e}")
        return []

